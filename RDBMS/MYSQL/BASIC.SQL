---------------基于mysql5.7+---------------

-- 查看连接状态
show processlist;

-- 重新设置连接(不需要重连和权限认证),防止长连接过长,内存OOM导致mysql重启
mysql_reset_connection;

-- redo log是物理日志,只存在innodb引擎中,其他mysql引擎中不存在,binlog是逻辑日志,是mysql server层的实现,记录操作语句.
-- 设置redo log每一次事务都持久化到硬盘
innodb_flush_log_trx_commit = 1

-- 设置bin log每一次事务都持久化到硬盘
sync_binlog = 1


-- mysql的两种视图
-- 1.view:利用create语法结合查询语句结果集生成的虚拟表.
-- 2.consistent read view:一致性视图,是多版本并发控制(MVCC)用得到一致性视图,用于实现读提交read-committed,
-- 可重复读repeatable-read的两种隔离级别.

------------------------------------- 事务隔离 -------------------------------------
-- 读未提交(read uncommitted):
-- 读提交(read committed): 每个sql开始执行时创建视图(多个事务共享数据??)
-- 可重复读(repeatable read): 事务启动时创建视图(MVCC视图),整个事务期间只使用这个试图(通过回滚日志实现??)
-- 串行化(serializable): 通过加锁方式实现,读写冲突时需要等待

-- 对于可重复读,查询只承认在事务启动前已经提交的数据,而读提交,只承认在语句启动前已经提交的数据

-- 查看mysql隔离级别
show variables like 'transaction_isolation';

-- oracle默认隔离级别是'读提交',因此oracle迁移应用到mysql时,为了保证数据库隔离级别一致,需要设置MySQL启动参数
-- mysql默认是REPEATABLE-READ
transaction_isolation = READ_COMMITTED

-- 长事务,会保留这个事务中的任意数据,也即是老旧试图,会占用大量空间,同时占用锁资源,尽量避免长事务
-- 这个命令是关闭自动提交,当执行任意语句,默认就会开启一个事务(不需要显式begin),并且不自动提交,直到执行commit/rollback之前,这就是一个长事务
set autocommit = 0

-- 只有使用begin/start显式开启事务(如果没有显式开启则自动提交??),然后使用commit提交,如果一个业务频繁使用事务,可以使用
-- commit work and chain代替commit表示提交这个事务,并且启动下一个事务,省去主动执行begin/start的语句交互
set autocommit = 1

-- 查找超过60s的事务
SELECT * FROM information_schema.innodb_trx WHERE TIME_TO_SEC(TIMEDIFF(NOW(),trx_started)) > 60;

-- begin/start如下的命令并不是事务的起点,直到执行第一个sal语句才会真正开启事务.
begin/start transaction
-- start ..如下是立刻开启一个事务(注意,这是创建一个持续整个事务的一致性快照,但是如果数据库隔离级别时读提交,那就相当于普通的开启事务,等价于上面的效果)
start transaction with consistent snapshot

-- InnoDB是为每个事务创建一个数组记录未提交的事务id,通过记录最小id低水位,以及系统最大id加1记作高水位,组成一致性视图,通过
-- 对比事务id和这个一致性视图得到版本的可见性.

-- mysql多版本数据(一致性视图),除了自己的更新是可见以外,还有三种情况:
-- 1.版本未提交,不可见
-- 2.版本已提交,但是是在视图创建后提交的,不可见
-- 3.版本已提交,而且是在视图创建前提交的,可见

-- 当前读:多数据版本可见性规则,除了上述,还有一条是当前读,即:更新数据都是先读后写,而这个读,只能读当前的值.
-- 除了update语句当前读,当select语句加锁时,也是当前读,如下分别读锁(S锁,共享锁),写锁(X锁,排他锁)
SELECT k FROM t WHERE id = 1 LOCK IN SHARE MODE;
SELECT k FROM t WHERE id = 1 FOR UPDATE;





------------------------------------- 索引 -------------------------------------
-- 主键索引:聚簇索引(clustered index)叶子节点存的是整行数据,主键查询方式,只需要查询这一颗B+
-- 普通索引:二级索引(secondary index),叶子节点存的是主键值,查询时需要先查一次普通索引树,获取到主键值,然后进行回表操作,去主键树查询数据
-- B+树维护:当数据页数据满的时候(中间插入数据造成的数据页满载,递增造成满载不会发生也分裂),发生页分裂,影响性能,同时空间利用
-- 率降低(刚满的时候平分到两个数据页),当新增数据时,依次递增是最好的情况,索引树不需要挪动,只需要往后追加,这也是建议使用自增id
-- 作为主键的原因,避免插入的主键落入中间位置,后续的数据都要做出偏移,同时也不会触发页分裂;而业务逻辑字段不容保证有序插入,同时
-- 非主键索引的叶子都是主键值,那么主键值字节数越小,整体空间就越小,而自增id往往是int/bigint比string等其他业务字段节省空间.

-- 使用业务字段做主键的场景:只有一个索引,且是唯一索引,这个时候考虑主键索引

-- 索引优化:本质上都是减少回表操作
-- 1.覆盖索引:例如根据k查询主键id的时候,下面的语句,不需要回表操作,因为k索引树中的叶子节点上就是id主键值,减少回表,树的搜索次数
--          同时根据业务场景,利用联合索引实现覆盖索引,也可以减少回表操作.
SELECT * FROM t WHERE k BETWEEN 3 AND 5;
SELECT id FROM t WHERE k BETWEEN 3 AND 5;

-- 2.最左前缀:建立联合索引时候,第一原则是通过调整顺序实现最左原则,可以减少维护一个索引,当既有联合索引同时都涉及到各自的单独索引
--          这个时候空间原则有限,选择字段长度小的单独索引,大的作为联合索引:如 a>b 则联合索引(a,b),单独索引b

-- 3.索引下推:mysql5.6+引入了pushDown下推优化,当联合索引中的字段都在条件中时,会对第二个字段做出过滤,如:联合索引(author,age)
--          这个时候使用索引获取author是F开头的,然后进一步判断联合索引中的age字段做出过滤,5.6之前的版本不会对age字段做
--          过滤,会查出所有F开头的主键,然后逐个去主键B+中比对,而5.6之后会优先过滤出age字段,减少去主键B+的回表操作.
SELECT * FROM t WHERE author LIKE %F% AND age > 10;


-- 普通索引和唯一索引
-- 1.查询性能:普通索引查询时,当查到k=5时,需要继续对比下一个记录,直到不等于5;唯一索引因为唯一性,查到k=5时就会停止搜索;因为InnoDb
--         时按数据页为单位来读写的,所以不考虑特殊情况,比如k=5在数据页的最后一条,那么普通索引需要重新读取下一页数据,正常情况
--         下,k=5左右的数据都在同一个数据页,那么普通索引的多做的一次检索,只是内存中一次指针寻找和对比,所以与唯一索引多出来
--         的这次检索性能微乎其微,因此两者两者查询性能一致.
SELECT id FROM t WHERE k = 5;

-- 2.更新性能:
-- 2.1:要更新的目标页在内存中,那么普通索引和唯一索引性能几乎一致,普通索引是直接更新到内存,而唯一索引只是先判断一下唯一冲突,
--     然后更新到内存,事务完成,后台有进程merge更新,最后刷回物理数据.
-- 2.2:要更新的目标不再内存中,那么普通索引性能更好,因为唯一索引需要先读取数据页到内存,这就涉及到磁盘操作随机IO访问,这是数据库
--     成本最高的操作,而普通索引只是更新到change buffer中,减少了磁盘的随机访问.

-- change buffer:当有更新操作时,如果要更新的数据页恰好在内存中,则直接更新,如果不在内存中,正常情况下需要读取所需的数据页到
--       内存中,但是change buffer机制是把更新语句缓存在buffer中,等到下次有查询语句执行读取数据页时,再更新实际内容,这也
--       就是merge过程,同时后台线程也会定时merge,数据库正常shutdown过程中,也会执行merge操作,这也就相当于批量执行的效果
--       减少了读磁盘的次数,提高了语句执行速度,同时读入内存时要占用buffer pool,所以一定程度可以提高内存利用率.(唯一索引在
--       执行前会先判断是否违反唯一性约束,会首先读取对应的数据页到内存,既然肯定会读取数据页,那么就直接更新数据,就没必要change
--       buffer机制,因此只有普通索引可以使用,当然buffer也是用使用buffer pool的内存,因此也不能无限增大,如下参数设置表示
--       最多占用50%)
innodb_change_buffer_max_size = 50

-- change buffer是在merge的时候真正更新数据,因此merge之前缓存的变更记录越多,收益就越大,因此对于写多读少的业务场景比较好,
--       常见的就是账单类,日志类系统,而相反的是更新之后就需要查询的场景,则会立刻触发merge过程,这种不会减少随机访问IO次数,反
--       而增加了维护buffer的代价,这种业务场景下就不适用了,应该关闭change buffer机制.





------------------------------------- 锁 -------------------------------------
----------- 库级别锁:
-- 1.mysql提供了一种全局读锁(FLWRL),让整个数据库处于只读状态(不能有其他更新等操作),典型应用场景就是做全库的逻辑备份.
flush tables with read lock

-- 2.mysqldump使用参数single-transaction可以开启一致性事务,等同于可重复读的事务隔离级别,使用的是一致性视图,不影响
--   其他数据更新,但是要求是全库的所有表都要支持这个事务的隔离级别,比如MyISAM就不支持事务,因此不能使用.

--3.设置readonly参数可以让全库进入制度状态,但是缺点是当客户端出现异常是,数据库仍然保持readonly状态,而flwrl能自动释放全局锁
set global readonly = true

----------- 表级别锁:
-- 1.表锁,使用lock tables .. read/write语法实现,使用unlock tables解锁;如下,作用是限制其他线程写1,读写2,但是当前线程
--   也只能读1,读写2,不允许访问其他表,在没有出现行级锁之前,这种控制并发访问的粒度较大,影响面较广.
lock tables t1 read, t2 write

-- 2.另外一种是MDL(meta data lock)不需要显示使用,mysql5.5+引入,在访问一个表的时候会自动加上对应的读锁,写锁;避免当一个
--   线程读取数据时,另外一个线程修改了表结构,这样读取的数据就会与表结构不对应.读锁之间不互斥,读写锁以及写锁之间互斥,因此当执行
--   长事务的时候,另外的线程申请修改表结构等写锁会阻塞直到前一个事务执行完,这时第三个线程继续申请读锁,这个时候会阻塞直到写锁
--   完成释放,如果这个时候频繁请求读锁,会造成数据库线程爆满而挂掉.

-- 因此修改表结构要注意,长事务问题,查询innodb_trx表查看长事务kill掉,同时也根据数据库支持情况结合alisql的wait等语法尝试
-- 设置等待时间,一定时间内如果可以获取写锁就正常执行,否则就释放这个写锁,不阻碍后续的读锁,之后再不断尝试重复执行语句.
ALTER TABLE t NOWAIT ADD COLUMN ...;
ALTER TABLE t WAIT N ADD COLUMN ...;

----------- 行级别锁:
-- 1.innodb引擎支持行级别,MYISAM不支持;行锁是需要的时候才加上,但是不并不是执行完立刻释放,必须等事务结束才释放.
--   两阶段锁协议:行锁需要等待事务结束才释放,eg:一个事物中更新两行数据,第一行更新完不会释放第一个行锁,必须等等二行结束,同时
--   事务提交之后才释放这个两个行锁.因此当出现并发更新同一行的业务时,一定要将事务中其他不涉及并发的sql先执行,而涉及并发更新
--   同一行,就会竞争行锁,因此放到事务的最后执行,减少等待锁的时间.

-- 2.死锁,当出现循环依赖时发生死锁,eg:事务a执行更新id为1的语句时,事务b更新id为2的语句,紧接着事务a又申请更新id为2,事务b申请
--   更新id为1,这时事务a,b发生循环依赖发生死锁.
-- 2.1死锁超时策略:默认50s,等待超过50s第一个线程会超时退出,释放锁;但是这个超时时间过长,对于线上服务不可接受,同时又不能设置过
--   小,因为正常的不是死锁的业务可能被提前释放,造成误伤.
innodb_lock_wait_timeout = 50
-- 2.2死锁检测策略:默认开启,主动检测死锁当发现死锁的时候,回滚某一个线程事务;但是检测死锁的性能消耗较大,因为每发生一个事务被锁,
--   就要发起检测,假设1000个并发同时更新,那么死锁检测就是100w次,最终cpu爆炸.
indodb_deadlock_detect = on
-- 2.3因此死锁处理要具体看业务适当选择,如果确保业务没有死锁可以关掉死锁检测,同时可以考虑正常开启死锁检测,但是手动控制并发量,例如
--   通过中间件实现,也可以考虑将热点行数据分散到多行处理等


-- mysql设置慢查询,win10在my.ini配置文件中添加如下配置:
-- # 表示日志存储的方式.FILE将日志存入文件,默认是FILE,另外可以取值TABLE,表示将日志存入数据库表,当然也可以两者都选择,
-- # 配成FILE,TABLE.需要注意的是将日志记录到数据库表中,需耗费更多的系统资源,建议优先记录到文件.
log-output=FILE
-- # 当配置将日志记录到文件中时,这个参数可以指定文件名,默认路径在配置文件的同级的data目录下.
slow_query_log_file="slow.log"
-- # 慢查询日志的开关,1表示开启,0表示关闭
slow-query-log=1
-- # 慢查询日志的阈值,花费时间超过这个值的sql语句才会被记录,单位是秒
long_query_time=0

-- force index(a):强制使用索引a
SELECT * FROM t1 force index(a) where a between 1000 and 2000;


-- 查看索引信息,其中cardinality就是基数,代表索引的区分度,也就是索引上的不同值,越大越好.索引基数是mysql抽样计算出来的,默认
--   选择n个数据页，计算页面上的不同值，乘以这个这个索引的页面数就得到索引的基数，表的数据会持续更新，索引信息会等到变更的行数
--   超过1/m的时候重新做索引统计。
show index from t1;
-- 1.参数on表示统计信息持久化到内存：n=20，m=10.
-- 2.参数off表示统计信息储存在内存中：n=8，M=16.
innodb_stats_persistent = ON/OFF

-- 重置索引统计信息
analyze table t1;

-- mysql是优化器来统计要扫描的行数，explain中的rows就是优化器计算的sql执行扫描行数，有时候会选错索引，计算出来的行数就会有问题
--   通常优化器选择扫描行数较少的执行方案,但是具体还会考虑到回表操作,最终得出是使用主键索引还是是用普通索引.

-- 绝大对数优化器都能找到正确的索引,对于少数误判索引,以及索引选择异常解决(参考EXAMPLE.SQL案例):
-- 1.通过analyze语法可以重新统计索引信息；
-- 2.强制制定index(但是如果索引名字变更,后续不好维护)
-- 3.利用sql语法在保证结果集不变的情况下,诱导优化器选择选择索引
-- 4.尝试删除索引,而选择新建一个更合适的索引
